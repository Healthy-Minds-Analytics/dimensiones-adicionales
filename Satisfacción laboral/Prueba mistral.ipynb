{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "MODEL_PATH = \"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "def descargar_modelo():\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"Descargando modelo Mistral...\")\n",
    "        with requests.get(MODEL_URL, stream=True) as r:\n",
    "            total = int(r.headers.get('content-length', 0))\n",
    "            with open(MODEL_PATH, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True, desc=\"Descargando modelo\") as pbar:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "    else:\n",
    "        print(\"Modelo ya disponible en caché.\")\n",
    "\n",
    "def generar_interpretacion(media_intrinseca, media_extrinseca, media_general):\n",
    "    llm = Llama(model_path=MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "    resumen = (\n",
    "        f\"- Satisfacción intrínseca: {media_intrinseca}\\n\"\n",
    "        f\"- Satisfacción extrínseca: {media_extrinseca}\\n\"\n",
    "        f\"- Satisfacción general: {media_general}\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"Actúa como un psicólogo organizacional. A partir de las siguientes medias de satisfacción laboral, \"\n",
    "        \"genera una breve interpretación indicando fortalezas y oportunidades de mejora:\\n\\n\"\n",
    "        f\"{resumen}\\n\"\n",
    "        \"Interpretación:\"\n",
    "    )\n",
    "\n",
    "    respuesta = llm(prompt, max_tokens=300, stop=[\"\\n\\n\"])\n",
    "    interpretacion = respuesta[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    return interpretacion\n",
    "\n",
    "def cargar_prompts(path=\"prompts.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "prompts = cargar_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando modelo Mistral...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando modelo: 100%|██████████| 4.37G/4.37G [12:32<00:00, 5.81MB/s]\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "descargar_modelo()\n",
    "\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048)\n",
    "\n",
    "prompts = cargar_prompts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado medio realista creado por ChatGPT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultado medio realista creado por ChatGPT\\n\")\n",
    "media_general = 59.76\n",
    "resumen = f\"- Media de satisfacción general: {media_general}\"\n",
    "prompt = prompts[\"general\"].replace(\"{resumen}\", resumen)\n",
    "\n",
    "output = llm(prompt, max_tokens=300, )#stop=[\"\\n\\n\"])\n",
    "interpretacion = output[\"choices\"][0][\"text\"].strip()\n",
    "print(interpretacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado bastante bueno\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultado bastante bueno\\n\")\n",
    "media_general = 85.15\n",
    "resumen = f\"- Media de satisfacción general: {media_general}\"\n",
    "prompt = prompts[\"general\"].replace(\"{resumen}\", resumen)\n",
    "\n",
    "output = llm(prompt, max_tokens=300, stop=[\"\\n\\n\"])\n",
    "interpretacion = output[\"choices\"][0][\"text\"].strip()\n",
    "print(interpretacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado bastante ideal\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultado bastante ideal\\n\")\n",
    "media_general = 101.91\n",
    "resumen = f\"- Media de satisfacción general: {media_general}\"\n",
    "prompt = prompts[\"general\"].replace(\"{resumen}\", resumen)\n",
    "\n",
    "output = llm(prompt, max_tokens=300, stop=[\"\\n\\n\"])\n",
    "interpretacion = output[\"choices\"][0][\"text\"].strip()\n",
    "print(interpretacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado malo\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultado malo\\n\")\n",
    "media_general = 35.24\n",
    "resumen = f\"- Media de satisfacción general: {media_general}\"\n",
    "prompt = prompts[\"general\"].replace(\"{resumen}\", resumen)\n",
    "\n",
    "output = llm(prompt, max_tokens=300, stop=[\"\\n\\n\"])\n",
    "interpretacion = output[\"choices\"][0][\"text\"].strip()\n",
    "print(interpretacion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat-laboral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
